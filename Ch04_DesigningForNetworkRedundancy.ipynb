{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to this module on Designing for Network Redundancy. We will begin this module with the discussion about Azure load balancers. We'll see the different options available, whether it is internal communication only with the use of a private load balancer or access to our back‑end application from the internet using a public load balancer. We will also see how both load balancing options can be used concurrently to provide a solid network infrastructure for your organization. We will also cover other load balancing solutions, such as the Application Gateway, which can distribute traffic for your web applications. We will discuss high availability with Traffic Manager and see how this feature can respond to your global DNS‑based load balancing needs. We will learn how an organization can extend its on‑premises environment to the Azure Cloud Services while keeping high availability in mind, whether it is by using Azure ExpressRoute or other VPN options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will see how to create and secure our virtual network through the use of a network security group. We will see how servers should be placed within virtual networks so to benefit from features such as availability zones. We will complete this module with virtual network peering and learn how to connect two different virtual networks together from the Azure portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing for High Availability with Azure Load Balancers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000005.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When designing for high availability in Azure, it is difficult not to talk about load balancers. Load balancing is very simple to set up, and it's a great way to provide highly‑available web applications to your organization within a region. Load balancers also support availability zones. If you're planning on deploying Azure load balancers within availability zones, the first thing you will want to make sure is that the region you're targeting for your deployment actually supports availability zones. A load balancer will operate at the transport layer or, if you prefer, at the OSI model layer 4. What this means is that TCP and UDP traffic is routed based on the following information, the source IP address and port and the destination IP address and port."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000006.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load balancers evenly distribute incoming traffic across backend servers based on load balancing and rules and health checks. Such a load balancing rule could be based on session persistence, and the health check could be based on the ability of the backend server to respond to HTTP requests. It gives you the ability to easily scale your applications either by adding or removing servers in the backend. Azure load balancers offer support for both TCP and UDP protocols such as SMTP, HTTP, or HTTPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000007.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Azure, you will find two types of load balancers. The first type is an external or public load balancer. A public load balancer will provide access to backend resources such as virtual machines residing inside your internal network to incoming traffic from the internet. The second type is an internal or private load balancer. It can be used when load balancing is done inside your internal network. There are no public IP addresses to manage, only private IPs are required. In some cases, both public and internal load balancers can be used within the same deployment for delivering applications to your organization. Let's see what this could look like with an example of such a design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000008.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, a client will connect to a website, the URL for this websites translates to the public load balancer IP address. The load balancer will then redirect a connection to one of the web servers depending on the load balancing rules. Now let's say those web servers are sending or retrieving their data from databases posted on multiple SQL servers. The client's request is sent to the internal load balancer, which will be redirected to the first available server, again depending on the load balancing rules. So what this means in terms of persistence or high availability is that you could lose one web server, and the public load balancer will be aware of this, and will stop sending client requests to the server. You could also lose one of the database servers, and the internal load balancer will know about it and will stop sending communication to this server until the issue is resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all done by the use of health probes. You can configure the probes to verify that services to the backend are healthy. If for whatever reason a server becomes unavailable based on the probe settings, the load balancer will stop sending requests to the specific server until the probe informs the load balancer the server is back online. From the client's perspective, this is all transparent. We can conclude that load balancers are a very good solution when designing high availability for your applications within a single region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Gateway and Traffic Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to provide high availability to web applications is the Azure Application Gateway. It can load balance web traffic based on more information than just a source and destination IP addresses and ports. The Application Gateway can route requests to a set of servers, referred to as a pool, based on the incoming URL. For example, as we can see in this slide, a request for globomantics.com/archives and globomantics.com/videos will be routed by the Application Gateway to two different pools of servers. This gives you a lot of control on where traffic should be directed and how the resources should be allocated. The backend servers hosting video content most probably will be consuming more resources than servers hosting the archived documents. Being able to separate both of these environments in different pools of servers and simply configure the Application Gateway to redirect traffic based on the incoming URL is really an easy way to provide highly‑available applications to your users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000012.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Application Gateway offers some other very interesting features, such as WAF, or Web Application Firewall, rules to secure your perimeter from traffic trying to access your internal servers. In most cases, such traffic comes from the internet, so being able to protect your internal perimeter is crucial. You can also use SSL offload and SSL policy. The SSL offload feature can be used to reduce SSL operations such as encryption and decryption with traffic by the web service themselves. It also reduces the effort required when updating or installing SSL certificates as you do not need to install any certificates on the backend tier. The backend servers can communicate with the Application Gateway using cleartext HTTP, while the channel between the Application Gateway and the clients will be done securely using HTTPS. Again, this is all transparent for the end user. From their perspective, they are communicating with a web server in a secure way, which they are actually doing. Another feature supported by the Application Gateway is cookie‑based session affinity. This is helpful when dealing with applications used for e‑commerce transactions. The client session will remain on the same server based on the cookie created when the session was initiated. By using this feature, you make sure the client can complete its transactions on the initial server in the event the session was interrupted. You can see the Application Gateway as a more advanced load balancing solution.\n",
    "\n",
    "If your scenario requires you to load balance DNS‑based traffic, then Traffic Manager can provide such a solution. Traffic Manager is a global load balancing solution that can distribute traffic across Azure regions using DNS. You define internet‑facing services known as endpoints. For example, you could have two endpoints, one located in the East US region and the other one in the West US region. Once Traffic Manager is configured with these two endpoints, users requesting access to either of them will be routed to the most efficient location based on the routing policy defined within Traffic Manager. Such a policy could be based on latency or on a specific subnet, making sure a set of IP addresses are redirected to a specific pool of servers.\n",
    "\n",
    "Another a global load balancing solution is the Azure Front Door. This feature provides DSA, or dynamic site acceleration, as well as global HTTP load balancing. In today's world, users are expecting a fast and reliable experience when they access web content. More and more, web applications offer real‑time interactions between the user and the application itself. A slow response time from your environment can lead to the user leaving the page and move to another site. This could result with a negative financial impact for your company. Azure Front Door can provide such a fast and reliable user experience by looking at the incoming HTTP request and routing the traffic to the nearest region. All of this is based on the specified URL, hostname, and configured rules. Whenever Front Door determines an issue, let's say, with latency for a specific region, it will then route traffic to the other fastest, or healthy, region. This grants the users with the experience they are expecting when interacting with your web applications. Now, let's jump to the next demo and configure a load balancing solution for our web servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending to Azure and High Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending an on‑premises environment to Azure is something that a lot of organizations are considering these days. In fact, there's a lot of reasons to do so. It could start by extending Active Directory Domain Services or migrating some internal databases to Azure. Some organizations will also decide on using Azure as a disaster recovery site. Being able to connect both your on‑prem and cloud infrastructure together brings a lot of benefits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000015.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what are some connectivity options to extend your on‑premises environment to Azure. We'll start with virtual private network or, if you prefer acronyms, VPN. In Azure, you will find the Azure VPN gateway, which can be used in different ways. The first option could be point‑to‑site, or P2S, VPN gateway connections. This is a good solution if you only have a handful of users that need to connect to your Azure environment. It is also a good option for users working remotely either from home, auto‑rooms, or networks that are not connected to your on‑prem environment. For Windows users, a package needs to be installed in the user's computer in order to initiate the connection to Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000016.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second option, you could choose to use site‑to‑site, or S2S, VPN gateway connections. This will connect your on‑premises network to an Azure virtual network using an IPsec/IKE VPN tunnel. In order to do so, you will need a compatible VPN device deployed within your environment. You can search for a list of compatible devices from the docs at the microsoft.com website. This device will also need to be assigned an external facing public IPv4 address. Once the VPN tunnel is up and running, users do not need to initiate any software from the local computer to access the Azure environment.\n",
    "\n",
    "As a third option, you could decide on using Express Route. This is a high‑speed connection using a circuit to connect your on‑premises network to Azure. Express Route does not use the internet, so it is faster and more secure. So if using the internet to connect to your Azure network is of any concern, you might want to take a look at this solution. Remember, Express Route requires a physical circuit, which can be obtained through third parties known as connectivity providers. At the time of recording this video, Express Route bandwidth could be from 50 Mbps to 10 Gbps. As Azure is in constant expansion, you should expect that more bandwidth will be available over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000017.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to provide high‑availability access through your Azure virtual network, and this is a good thing. An Azure site should be considered as important as any other site you add to your network infrastructure. You need to think about redundancy and failover. It is possible to design our network architecture using a combination of both Express Route and a site‑to‑site VPN failover. Users would connect to the Express Route circuit as a primary route. In the event this circuit becomes unavailable, Azure will fail over and route users' requests to the site‑to‑site VPN tunnel. This is a topology you might want to think over if you're responsible for providing high available access to your Azure resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual Network Peering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000019.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network foundation of your Azure environment starts with virtual networks, or VNets. This module will cover how to provide high availability within your networking environment. A virtual network is an isolation boundary. You create a VNet within an address space and then create a subnet within this address space. You then place servers in a virtual network. VMs within the same VNet can communicate with each other, but by default, virtual machines in one VNet cannot communicate with other VMs in another virtual network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000020.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be a case where you will need virtual machines in one VNet being able to communicate with other VMs located in another virtual network. As discussed earlier, by default, there is no such connectivity. For example, you are running a dev environment within its own virtual network, but virtual machines from this dev environment, at some point, may need to be migrated to production, which resides in another VNet. There are different ways to do this, but a very simple way of doing it is virtual network peering. VNet peering gives you the possibility to seamlessly connect two VNets together. Also, you can connect virtual networks that exist in different subscriptions.\n",
    "\n",
    "There's two types of VNet peering. First, virtual network peering, which will be connecting virtual networks inside the same Azure region. But, this will not protect you from a regional outage. This brings us to the second type of VNet peering, global virtual network peering, which will be connecting virtual networks across different Azure regions. This will bring both resiliency and high availability to your network infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000021.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can protect your virtual network from incoming traffic with the use of a network security group, also known as NSG. You can bind the network security group to a virtual network interface, or VNIC, or you can bind the NSG at a subnet level. By binding at the subnet level, you make sure all servers you add to the virtual network will inherit the same inbound and outbound rules defined within your network security group. I find it a lot easier to manage than binding the NSG to the network interface. Enough said, let's now see how to deploy this solution from the portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Working with Virtual Networks and Network Security Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In this demo, we will create two virtual networks, and we will create a network security group, or NSG, to control inbound and outbound traffic to the new VNets. From the portal, go to the Virtual networks blade. From here, you can view and manage existing virtual networks. At this point, we do not have any, so let's create one by clicking on Create virtual network, select your Resource group or create a new one. Then we'll need to give this new virtual network a name, which will be vnetA. The Region for vnetA will be East US. Click on Next to configure IP addresses. I'll keep the default address space of 10.0.0.0/16. This provides more than 65,000 addresses. It should be more than enough for this demo. As for the subnet, I'll select default subnet address range of 10.0.0.0/24. Under the Security section, I'll also keep the default as we will set this up later on in this demo. Click on Review + create and wait for the deployment to complete. Back to the Virtual networks blade, we can see vnetA now appears. We'll repeat the same steps to create our second virtual network. Click on Add, select your Resource group, and provide a name. This virtual network will be called vnetB. Region will be East US. Now, let's configure IP addresses. Again, let's keep the default address space of 10.1.0.0/16 and keep the default subnet address range of 10.1.0.0/24. Click on Review + create to send the deployment job to Azure. Once the deployment is complete, open the Virtual networks blade. From here, I can confirm both virtual networks were created. I can see vnetA and vnetB. Now, let's secure our virtual networks. In order to do so, we will create a network security group, or NSG, which will stand as a firewall for our virtual networks. Click on Create a resource and search for network security group. From the Overview, we can read that a network security group is a layer of security that acts as a virtual firewall for controlling traffic in and out of virtual machines, your network interfaces, and subnets. That's exactly what we are looking for, so let's click on Create, select your Resource group, and provide a name for the security group. In my case, it will be web‑nsg. I'll select East US for the Region. Next, click on Review + create to start the deployment. I'll navigate to my Resource group, and I can now see I have both vnetA and vnetB, as well as a new network security group. Select web‑nsg to see its properties. We can see this network security group is not associated with any subnets or network interfaces. We're going to fix this by associating this network security group to the subnets we created earlier. On the left, click on subnets. At the top, click on Associate, select vnetA with its default subnet, click OK to complete the task. I'll repeat the same steps to associate vnetB as well to the network security group. I'll select the default subnet and hit OK. We can now see both virtual networks and their subnets are now associated with this network security group. If we take a look at the Inbound rules, we can see that any port from any protocol is allowed for our virtual networks. If we look at the outbound, security rules, we can see the settings are just the same. But remember, a virtual network is an isolation boundary. No traffic will be allowed between both VNets until you enable peering. This is exactly what we are going to do in the next clip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Enabling Virtual Network Peering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In this demo, we will deploy two virtual machines within the virtual networks we created in the previous video. We will complete this demo by peering the two VNets so both virtual machines are able to communicate with each other. We'll start by deploying a virtual machine to our first virtual network. From the Virtual machines blade, click on Create virtual machine. I'll select my resource group, LABM5, and provide a name for the virtual machine, which will be WEB1. I'll then change the region from Central US and select East US instead. I will deploy this virtual machine within an availability zone, and I will select availability zone 1. The image I will be using is the Ubuntu Server 18.04 LTS. I've changed the size of this VM to keep this lab on a low budget. I'll provide a username and password for the administrator account, and my account name is sysadmin. I'll select None for the public inbound ports. We can skip the Disks section to go directly to the networking configuration. Make sure the virtual network is vnetA and that its default subnet is selected. We do not need any public IPs, so we can select None here. Under NIC network security group, we can see that this subnet is already associated to a network security group, web‑nsg. All firewall rules should be managed by this network security group instead of creating one NSG for each network interface attached to your VMs. We're now ready to get this first VM running, so click on Review and create and wait for the deployment to complete. Once it is completed, go back to the Virtual machines blade to create a second VM. We'll call it WEB2, and we'll be using availability zone 3. I changed the VM size and provided a username and password for the admin account. Let's switch to the Networking tab. Select vnetB and keep the default subnet. I'll also remove the public IP. Again, we can see the subnet is managed by our web‑nsg security group. We're all good, so I'll just click on Review and create to start the deployment. Once it is complete, go back to the Virtual machines section. Let's review the settings for WEB1. We can set the private IP, which is 10.0.0.4. We can also see the virtual network and subnet this VM is part of. We can also confirm this virtual machine is running in East US in availability zone 1. Let's check out the same settings for WEB2. We can see the private IP here, which is 10.1.0.4. We see it's part of the vnetB virtual network. The VM is running in East US within the availability zone 3. Let's drill down to the serial console and connect to the virtual machine. After providing a username and password, I run the ifconfig command to view the network configuration. I can confirm the IP address is 10.1.0.4. Let's see if we're able to communicate with WEB1 in the other virtual network, vnetA, using the ping command. I remember the IP address of WEB1 is 10.0.0.4, so I'll just ping that IP. As we can see, there is no communication between both virtual machines. In order to establish communication between both VNets, we'll need to enable peering. Using another tab, I'm logged into the portal under the Virtual networks section. Let's select vnetA, drill down until you find Peerings, and then click on Add to start the peering configuration for our two virtual networks. We'll need to provide a name for the peering. I'll choose a name that will reflect the purpose of this peering, so it will be vnetA to vnetB. Under Virtual network, I'll select the only VNet I can choose from, which is vnetB. I'll keep the same standard for naming the peering from vnetB to vnetA. Under Configuration, make sure access from both VNets is set to enabled. Click on OK to confirm the peering. We can confirm that vnetA is now connected with its peer, vnetB. Let's switch to the VM console. We can now confirm virtual machines can now communicate with each other, even though they are located in a different virtual network and subnet. Back to the vnetA peering section, I will click on the peering to see more details. We can see the peering status and details about the other peer. From here, you can delete a peering, or you can simply disable it instead by selecting Disabled. Simply disabling the peering is often easier than deleting it and having to recreate it later on. As you can see, enabling peering between your VNets is a very simple task. Now, let's jump to the next clip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Module Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/agungwahyudi-public-files/azure_high_availability-4-000022.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is already the end of this module on Designing for Network Redundancy. I hope that what we've covered here will help you better understand how you should design your network infrastructure, starting with load balancing solutions, whether you decide on using a public or private load balancer or a combination of both. If a more advanced load balancing solution is needed, you could use Application Gateway to provide access to your web applications. You could then benefit from features such as SSL offload or cookie‑based session affinity. You could also protect your parameter using web application firewall rules, which are also available using an application gateway. We also covered how Traffic Manager can load balance DNS‑based traffic to offer a strong global load balancing solution. We've learned how to extend an on‑premises network infrastructure to the Azure Cloud either by using Azure VPN gateway, ExpressRoute, or a combination of the two. We concluded this module with a lab about virtual networks and how to enable VNet peering to allow communications to virtual machines running in different virtual networks. Now let's jump to the last module of this course, Designing for Application Redundancy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
